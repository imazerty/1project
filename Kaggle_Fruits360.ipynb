{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_Fruits360.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imazerty/1project/blob/master/Kaggle_Fruits360.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA7nKPe34I-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "  \"\"\"\n",
        "  This function prints and plots the confusion matrix.\n",
        "  Normalization can be applied by setting `normalize=True`.\n",
        "  \"\"\"\n",
        "  if normalize:\n",
        "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "      print(\"Normalized confusion matrix\")\n",
        "  else:\n",
        "      print('Confusion matrix, without normalization')\n",
        "\n",
        "  print(cm)\n",
        "\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes, rotation=45)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "\n",
        "  fmt = '.2f' if normalize else 'd'\n",
        "  thresh = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      plt.text(j, i, format(cm[i, j], fmt),\n",
        "               horizontalalignment=\"center\",\n",
        "               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTTyo7fcNGco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function\n",
        "from builtins import range, input\n",
        "\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# re-size all the images to this\n",
        "IMAGE_SIZE = [100, 100] # feel free to change depending on dataset\n",
        "\n",
        "# training config:\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "# https://www.kaggle.com/moltean/fruits\n",
        "# train_path = '../large_files/fruits-360/Training'\n",
        "# valid_path = '../large_files/fruits-360/Validation'\n",
        "train_path = '../large_files/fruits-360-small/Training'\n",
        "valid_path = '../large_files/fruits-360-small/Validation'\n",
        "# useful for getting number of files\n",
        "image_files = glob(train_path + '/*/*.jp*g')\n",
        "valid_image_files = glob(valid_path + '/*/*.jp*g')\n",
        "\n",
        "# useful for getting number of classes\n",
        "folders = glob(train_path + '/*')\n",
        "\n",
        "# look at an image for fun\n",
        "plt.imshow(image.load_img(np.random.choice(image_files)))\n",
        "plt.show()\n",
        "\n",
        "# add preprocessing layer to the front of VGG\n",
        "vgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)\n",
        "\n",
        "# don't train existing weights\n",
        "for layer in vgg.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "# our layers - you can add more if you want\n",
        "#If you wanted to use a Dense(a fully connected layer) you need to flatten the output\n",
        "x = Flatten()(vgg.output)\n",
        "predictions = Dense(len(folders), activation = 'softmax')(x)\n",
        "\n",
        "# create a model object\n",
        "model = Model(inputs = vgg.input, outputs = predictions)\n",
        "\n",
        "# view the structure of the model\n",
        "model.summary\n",
        "\n",
        "# tell the model what cost and optimization method to use\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
        "#rmsprop prevents gradients from blowing up\n",
        "\n",
        "# create an instance of ImageDataGenerator\n",
        "gen = ImageDataGenerator(\n",
        "  rotation_range=20,\n",
        "  width_shift_range=0.1,\n",
        "  height_shift_range=0.1,\n",
        "  shear_range=0.1,\n",
        "  zoom_range=0.2,\n",
        "  horizontal_flip=True,\n",
        "  vertical_flip=True,\n",
        "  preprocessing_function = preprocess_input #to rearrange the channels etc because vgg was trained using caffe \n",
        ")\n",
        "\n",
        "# test generator to see how it works and some other useful things\n",
        "\n",
        "# get label mapping for confusion matrix plot later\n",
        "\n",
        "test_gen = gen.flow_from_directory(valid_path, target_size = IMAGE_SIZE)\n",
        "print(test_gen.class_indices) #gives a dict where the key is the class name and the value is the class index\n",
        "#let's switch the key and value\n",
        "labels = [None] * len(test_gen.class_indices)\n",
        "for k, v in test_gen.class_indices.items():\n",
        "  labels[v] = k\n",
        "  \n",
        "# should be a strangely colored image (due to VGG weights being BGR) because we switched the channels\n",
        "#A DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labels.\n",
        "for x, y in test_gen:\n",
        "  print('min = {} and max = {}'.format(x[0].min(), x[0].max()) )#not 0-255\n",
        "  plt.title(label[np.argmax(y[0])])\n",
        "  plt.imshow(x[0])\n",
        "  plt.show()\n",
        "  break\n",
        "  \n",
        "# create generators\n",
        "train_generator = gen.flow_from_directory(\n",
        "train_path,\n",
        "target_size=IMAGE_SIZE,\n",
        "shuffle=True,\n",
        "batch_size=batch_size,\n",
        ")\n",
        "\n",
        "valid_generator = gen.flow_from_directory(\n",
        "  valid_path,\n",
        "  target_size=IMAGE_SIZE,\n",
        "  shuffle=True,\n",
        "  batch_size=batch_size,\n",
        ")\n",
        "\n",
        "# fit the model\n",
        "r = model.fit_generator(\n",
        "train_generator,\n",
        "validation_data=valid_generator,\n",
        "epochs=epochs,\n",
        "steps_per_epoch=len(image_files) // batch_size,\n",
        "validation_steps=len(valid_image_files) // batch_size,\n",
        ")\n",
        "#scikit-learn already has a confusion matrix but the problem is we need to pass it 2arrays targets and predictions which we don't have\n",
        "#we only have data generators which we can use to build the arrays we need\n",
        "#useful note: flow_from_directory never ends so we need to use the steps_per_epoch  to stop the infinite loop\n",
        "\n",
        "def get_confusion_matrix(data_path, N):\n",
        "  # we need to see the data in the same order\n",
        "  # for both predictions and targets\n",
        "  print(\"Generating confusion matrix\", N)\n",
        "  predictions = []\n",
        "  targets = []\n",
        "  for x, y in gen.flow_from_directory(data_path, target_size=IMAGE_SIZE, shuffle=False, batch_size=batch_size * 2):\n",
        "    #y is a one-hot encoded\n",
        "    #p is an array of proba\n",
        "    #we can turn these into a 1D array of labels by taking the argmax then concatenating with eisting targets and predictions\n",
        "    \n",
        "    p = model.predict(x)\n",
        "    p = np.argmax(p, axis=1)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    predictions = np.concatenate((predictions, p))\n",
        "    targets = np.concatenate((targets, y))\n",
        "    if len(targets) >= N:\n",
        "      break\n",
        "\n",
        "    cm = confusion_matrix(targets, predictions)\n",
        "    return cm    \n",
        "  \n",
        "cm = get_confusion_matrix(train_path, len(image_files))\n",
        "print(cm)\n",
        "valid_cm = get_confusion_matrix(valid_path, len(valid_image_files))\n",
        "print(valid_cm)\n",
        "\n",
        "# plot some data\n",
        "\n",
        "# loss\n",
        "#keras stores it as a history object when we call fit\n",
        "plt.plot(r.history['loss'], label= 'train loss' )\n",
        "plt.plot(r.history['val_loss'], label = 'val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#accuracy\n",
        "\n",
        "plt.plot(r.history['acc'], label = 'train acc')\n",
        "plt.plot(r.history['val_acc'], label = 'val acc')\n",
        "plt.legend()\n",
        "plt.show\n",
        "\n",
        "#confusion_matrix\n",
        "\n",
        "plot_confusion_matrix(cm, labels, title='Train confusion matrix')\n",
        "plot_confusion_matrix(valid_cm, labels, title='Validation confusion matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}